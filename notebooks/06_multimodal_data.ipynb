{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a7a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERAZIONE DATASET (LEXRANK) ---\n",
      "1. Elaborazione News...\n",
      "   Inizio News rilevato: 2017-08-18\n",
      "2. Elaborazione SPY...\n",
      "   Filtraggio SPY: 2017-05-20 -> 2023-12-28\n",
      "3. Unione Dataset...\n",
      "   Giorni con sentiment attivo: 1098 su 1663\n",
      "   Sentiment range: [-0.965, 0.933]\n",
      "4. Split Temporale...\n",
      "   Train: 1164 | Val: 249 | Test: 250\n",
      "   Date range Train: 2017-05-22 -> 2022-01-03\n",
      "   Date range Val:   2022-01-04 -> 2022-12-29\n",
      "   Date range Test:  2022-12-30 -> 2023-12-28\n",
      "5. Scaling...\n",
      "6. Creazione Sequenze...\n",
      "   Train: 1104 | Val: 189 | Test: 190\n",
      "   Input Shape: (1104, 60, 7)\n",
      "7. Salvataggio...\n",
      "   - Features: 7 (volume, open, high, low, close, adj close, sentiment_score)\n",
      "   - Sequenze: 60 timesteps\n",
      "   - Target: adj close\n",
      "   - Train shape: (1104, 60, 7) -> y: (1104,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "NEWS_CSV_PATH = 'C:/Users/angel/OneDrive/Desktop/ProgettoMeDL/sentiments/spy_news_sentiment_titolo.csv' \n",
    "OUTPUT_DIR = 'C:/Users/angel/OneDrive/Desktop/ProgettoMeDL/Financial_Forecasting_XAI/data_split_titolo' \n",
    "SPY_PATH = 'C:/Users/angel/OneDrive/Desktop/ProgettoMeDL/SPY.csv'\n",
    "\n",
    "LOOKBACK_WINDOW = 60\n",
    "TARGET_COLUMN = 'adj close'\n",
    "\n",
    "def create_sequences(data, lookback, target_idx):\n",
    "    \"\"\"Crea sequenze X,y da dati scalati\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i-lookback:i])\n",
    "        y.append(data[i, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    print(f\"--- GENERAZIONE DATASET (LEXRANK) ---\")\n",
    "\n",
    "    # 1. CARICAMENTO E PULIZIA NEWS\n",
    "    print(\"1. Elaborazione News...\")\n",
    "    df_news = pd.read_csv(NEWS_CSV_PATH)\n",
    "    \n",
    "    # Verifica colonne necessarie\n",
    "    required_cols = ['Date', 'sentiment_positive', 'sentiment_negative']\n",
    "    if not all(col in df_news.columns for col in required_cols):\n",
    "        raise ValueError(f\"CSV news deve contenere: {required_cols}\")\n",
    "    \n",
    "    # Parsing Date News\n",
    "    df_news['Date'] = pd.to_datetime(df_news['Date'], utc=True).dt.date\n",
    "    df_news['Date'] = pd.to_datetime(df_news['Date'])\n",
    "    \n",
    "    # Calcolo Sentiment (come in FinBERT)\n",
    "    df_news['sentiment_score'] = df_news['sentiment_positive'] - df_news['sentiment_negative']\n",
    "    \n",
    "    news_start_date = df_news['Date'].min()\n",
    "    print(f\"   Inizio News rilevato: {news_start_date.date()}\")\n",
    "    \n",
    "    # Raggruppamento giornaliero\n",
    "    daily_sentiment = df_news.groupby('Date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "    # 2. CARICAMENTO E FILTRAGGIO SPY\n",
    "    print(\"2. Elaborazione SPY...\")\n",
    "    df = pd.read_csv(SPY_PATH)\n",
    "    df.columns = df.columns.str.lower() \n",
    "    \n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(\"Colonna 'date' non trovata in SPY.csv\")\n",
    "        \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date', ascending=True)\n",
    "    \n",
    "    # CORREZIONE: Buffer basato su LOOKBACK_WINDOW\n",
    "    start_date_limit = news_start_date - pd.Timedelta(days=LOOKBACK_WINDOW + 30)\n",
    "    end_date_limit = '2023-12-28'\n",
    "    \n",
    "    print(f\"   Filtraggio SPY: {start_date_limit.date()} -> {end_date_limit}\")\n",
    "    \n",
    "    mask = (df['date'] >= start_date_limit) & (df['date'] <= end_date_limit)\n",
    "    df = df.loc[mask].copy()\n",
    "    \n",
    "    # Verifica continuità date\n",
    "    date_diffs = df['date'].diff().dt.days\n",
    "    max_gap = date_diffs.max()\n",
    "    if max_gap > 7:  # Più di una settimana di gap\n",
    "        print(f\" Gap massimo tra date: {max_gap} giorni\")\n",
    "    \n",
    "    # 3. MERGE\n",
    "    print(\"3. Unione Dataset...\")\n",
    "    df['date_clean'] = pd.to_datetime(df['date'].dt.date)\n",
    "    \n",
    "    df_merged = pd.merge(\n",
    "        df, \n",
    "        daily_sentiment, \n",
    "        left_on='date_clean', \n",
    "        right_on='Date', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # CORREZIONE: Solo fillna(0) per sentiment neutro\n",
    "    df_merged['sentiment_score'] = df_merged['sentiment_score'].fillna(0)\n",
    "    \n",
    "    # Pulizia colonne ridondanti\n",
    "    df_merged = df_merged.drop(columns=['date_clean', 'Date'], errors='ignore')\n",
    "    \n",
    "    # Statistiche\n",
    "    non_zeros = (df_merged['sentiment_score'] != 0).sum()\n",
    "    print(f\"   Giorni con sentiment attivo: {non_zeros} su {len(df_merged)}\")\n",
    "    print(f\"   Sentiment range: [{df_merged['sentiment_score'].min():.3f}, {df_merged['sentiment_score'].max():.3f}]\")\n",
    "\n",
    "    # 4. SPLIT TEMPORALE (PRIMA DELLO SCALING!)\n",
    "    print(\"4. Split Temporale...\")\n",
    "    financial_cols = ['volume', 'open', 'high', 'low', 'close', 'adj close']\n",
    "    sentiment_col = ['sentiment_score']\n",
    "    feature_cols = financial_cols + sentiment_col\n",
    "    \n",
    "    total_samples = len(df_merged)\n",
    "    train_split_idx = int(total_samples * 0.70)\n",
    "    val_split_idx = int(total_samples * 0.85)\n",
    "    \n",
    "    df_train = df_merged.iloc[:train_split_idx].copy()\n",
    "    df_val = df_merged.iloc[train_split_idx:val_split_idx].copy()\n",
    "    df_test = df_merged.iloc[val_split_idx:].copy()\n",
    "    \n",
    "    print(f\"   Train: {len(df_train)} | Val: {len(df_val)} | Test: {len(df_test)}\")\n",
    "    print(f\"   Date range Train: {df_train['date'].min().date()} -> {df_train['date'].max().date()}\")\n",
    "    print(f\"   Date range Val:   {df_val['date'].min().date()} -> {df_val['date'].max().date()}\")\n",
    "    print(f\"   Date range Test:  {df_test['date'].min().date()} -> {df_test['date'].max().date()}\")\n",
    "\n",
    "    # 5. SCALING (FIT SOLO SU TRAIN!)\n",
    "    print(\"5. Scaling...\")\n",
    "    \n",
    "    # Scaler per dati finanziari\n",
    "    scaler_financial = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_financial.fit(df_train[financial_cols].values)  # FIT SOLO SU TRAIN\n",
    "    \n",
    "    scaled_financial_train = scaler_financial.transform(df_train[financial_cols].values)\n",
    "    scaled_financial_val = scaler_financial.transform(df_val[financial_cols].values)\n",
    "    scaled_financial_test = scaler_financial.transform(df_test[financial_cols].values)\n",
    "    \n",
    "    # Scaler per sentiment\n",
    "    scaler_sentiment = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_sentiment.fit(df_train[sentiment_col].values)  # FIT SOLO SU TRAIN\n",
    "    \n",
    "    scaled_sentiment_train = scaler_sentiment.transform(df_train[sentiment_col].values)\n",
    "    scaled_sentiment_val = scaler_sentiment.transform(df_val[sentiment_col].values)\n",
    "    scaled_sentiment_test = scaler_sentiment.transform(df_test[sentiment_col].values)\n",
    "    \n",
    "    # Concatenazione\n",
    "    scaled_train = np.concatenate([scaled_financial_train, scaled_sentiment_train], axis=1)\n",
    "    scaled_val = np.concatenate([scaled_financial_val, scaled_sentiment_val], axis=1)\n",
    "    scaled_test = np.concatenate([scaled_financial_test, scaled_sentiment_test], axis=1)\n",
    "    \n",
    "    # Salva scalers\n",
    "    joblib.dump({\n",
    "        'financial': scaler_financial,\n",
    "        'sentiment': scaler_sentiment\n",
    "    }, os.path.join(OUTPUT_DIR, 'scaler.pkl'))\n",
    "    \n",
    "    # 6. CREAZIONE SEQUENZE\n",
    "    print(\"6. Creazione Sequenze...\")\n",
    "    target_idx = feature_cols.index(TARGET_COLUMN)\n",
    "    \n",
    "    X_train, y_train = create_sequences(scaled_train, LOOKBACK_WINDOW, target_idx)\n",
    "    X_val, y_val = create_sequences(scaled_val, LOOKBACK_WINDOW, target_idx)\n",
    "    X_test, y_test = create_sequences(scaled_test, LOOKBACK_WINDOW, target_idx)\n",
    "    \n",
    "    print(f\"   Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "    print(f\"   Input Shape: {X_train.shape}\")\n",
    "    \n",
    "    # CORREZIONE: Verifica shape\n",
    "    assert X_train.shape[2] == len(feature_cols), \\\n",
    "        f\"Expected {len(feature_cols)} features, got {X_train.shape[2]}\"\n",
    "    assert X_train.shape[1] == LOOKBACK_WINDOW, \\\n",
    "        f\"Expected lookback {LOOKBACK_WINDOW}, got {X_train.shape[1]}\"\n",
    "\n",
    "    # 7. SALVATAGGIO\n",
    "    print(\"7. Salvataggio...\")\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'X_train.npy'), X_train)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'X_val.npy'), X_val)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'y_val.npy'), y_val)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'X_test.npy'), X_test)\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'y_test.npy'), y_test)\n",
    "    \n",
    "    # Salva anche info sul dataset\n",
    "    metadata = {\n",
    "        'lookback_window': LOOKBACK_WINDOW,\n",
    "        'target_column': TARGET_COLUMN,\n",
    "        'feature_columns': feature_cols,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'train_date_range': (str(df_train['date'].min().date()), str(df_train['date'].max().date())),\n",
    "        'val_date_range': (str(df_val['date'].min().date()), str(df_val['date'].max().date())),\n",
    "        'test_date_range': (str(df_test['date'].min().date()), str(df_test['date'].max().date()))\n",
    "    }\n",
    "    \n",
    "    joblib.dump(metadata, os.path.join(OUTPUT_DIR, 'metadata.pkl'))\n",
    "    \n",
    "    print(f\"   - Features: {len(feature_cols)} ({', '.join(feature_cols)})\")\n",
    "    print(f\"   - Sequenze: {LOOKBACK_WINDOW} timesteps\")\n",
    "    print(f\"   - Target: {TARGET_COLUMN}\")\n",
    "    print(f\"   - Train shape: {X_train.shape} -> y: {y_train.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
